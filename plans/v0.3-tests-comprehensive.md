# Comprehensive Test Specifications
## v0.3 — Self-Improvement
### This Mind Does Not Exist

---

**Document Version:** 1.0  
**Status:** Draft  
**Coverage Target:** 90%+ for all new modules

---

## 1. Test Architecture

### 1.1 Test Stack

| Layer | Framework | Location |
|-------|-----------|----------|
| TypeScript unit tests | Vitest | `packages/core/src/**/*.test.ts` |
| TypeScript integration tests | Vitest | `packages/core/src/**/*.integration.test.ts` |
| Python unit tests | pytest | `packages/finetune/tests/` |
| API route tests | Vitest + supertest | `packages/core/src/server/**/*.test.ts` |
| UI component tests | Vitest + React Testing Library | `apps/web/**/*.test.tsx` |
| E2E tests | Playwright | `tests/e2e/` |

### 1.2 Test Utilities

```typescript
// packages/core/src/test-utils/index.ts

/**
 * Create an in-memory SQLite database for testing.
 */
export function createTestDb(): Database;

/**
 * Create a temporary directory for test files.
 * Automatically cleaned up after each test.
 */
export function createTempDir(): string;

/**
 * Create a mock LoRA adapter directory with valid files.
 */
export function createMockAdapter(dir: string, options?: {
  id?: string;
  baseModel?: string;
  metrics?: Partial<FineTuneMetrics>;
  corrupt?: boolean;
}): string;

/**
 * Create mock training data JSONL files.
 */
export function createMockTrainingData(dir: string, count: number): {
  trainPath: string;
  valPath: string;
};

/**
 * Mock the Ollama client for testing.
 */
export function createMockOllamaClient(): MockOllamaClient;

/**
 * Create a mock FineTuneManager that doesn't spawn real processes.
 */
export function createMockFineTuneManager(): MockFineTuneManager;
```

---

## 2. Feature 1: LoRA Fine-Tuning Pipeline Tests

### 2.1 FineTuneManager Unit Tests

**File:** [`packages/core/src/finetune/manager.test.ts`](../packages/core/src/finetune/manager.test.ts)

```typescript
import { describe, it, expect, beforeEach, afterEach, vi } from 'vitest';
import { FineTuneManager } from './manager';
import { createTestDb, createTempDir, createMockTrainingData } from '../test-utils';

describe('FineTuneManager', () => {
  let manager: FineTuneManager;
  let tempDir: string;
  let db: Database;

  beforeEach(() => {
    tempDir = createTempDir();
    db = createTestDb();
    manager = new FineTuneManager(/* ... */);
  });

  afterEach(async () => {
    await manager.cancelFineTune(manager.activeRunId ?? '').catch(() => {});
  });

  // ─── exportTrainingData ───

  describe('exportTrainingData', () => {
    it('exports traces with quality_score >= threshold', async () => {
      // Arrange: Insert 10 traces, 5 with score >= 8, 5 with score < 8
      await insertTestTraces(db, [
        { quality_score: 9.0 }, { quality_score: 8.5 }, { quality_score: 8.0 },
        { quality_score: 8.1 }, { quality_score: 9.5 },
        { quality_score: 7.9 }, { quality_score: 6.0 }, { quality_score: 5.0 },
        { quality_score: 7.5 }, { quality_score: 4.0 }
      ]);

      // Act
      const result = await manager.exportTrainingData({
        minQuality: 8.0,
        outputDir: tempDir,
        runId: 'test-run'
      });

      // Assert
      expect(result.count).toBe(5);
      const trainLines = readJsonlFile(result.trainPath);
      const valLines = readJsonlFile(result.valPath);
      expect(trainLines.length + valLines.length).toBe(5);
    });

    it('exports traces with user_rating >= threshold even if auto_score is lower', async () => {
      await insertTestTraces(db, [
        { quality_score: 6.0, user_rating: 9 },
        { quality_score: 9.0, user_rating: null }
      ]);

      const result = await manager.exportTrainingData({
        minQuality: 8.0,
        outputDir: tempDir,
        runId: 'test-run'
      });

      expect(result.count).toBe(2);
    });

    it('formats traces as Alpaca instruction/input/output', async () => {
      await insertTestTraces(db, [{
        query: 'Prove √2 is irrational',
        final_answer: 'Assume √2 = p/q...',
        templates_used: JSON.stringify(['proof-by-contradiction']),
        quality_score: 9.0
      }]);

      const result = await manager.exportTrainingData({
        minQuality: 8.0,
        outputDir: tempDir,
        runId: 'test-run'
      });

      const examples = readJsonlFile(result.trainPath);
      expect(examples[0]).toMatchObject({
        instruction: 'Prove √2 is irrational',
        input: expect.stringContaining('proof-by-contradiction'),
        output: 'Assume √2 = p/q...'
      });
    });

    it('deduplicates traces with the same query', async () => {
      await insertTestTraces(db, [
        { query: 'Same question', quality_score: 9.0 },
        { query: 'Same question', quality_score: 8.5 },
        { query: 'Different question', quality_score: 9.0 }
      ]);

      const result = await manager.exportTrainingData({
        minQuality: 8.0,
        outputDir: tempDir,
        runId: 'test-run'
      });

      // Should keep only the highest-quality duplicate
      expect(result.count).toBe(2);
    });

    it('splits data 90/10 train/val deterministically', async () => {
      await insertTestTraces(db, Array(100).fill({ quality_score: 9.0 }));

      const result1 = await manager.exportTrainingData({
        minQuality: 8.0, outputDir: tempDir, runId: 'run-1'
      });
      const result2 = await manager.exportTrainingData({
        minQuality: 8.0, outputDir: tempDir, runId: 'run-2'
      });

      const train1 = readJsonlFile(result1.trainPath);
      const train2 = readJsonlFile(result2.trainPath);

      expect(train1.length).toBe(90);
      expect(train2.length).toBe(90);
      // Same order (deterministic)
      expect(train1[0].instruction).toBe(train2[0].instruction);
    });

    it('throws when no eligible traces exist', async () => {
      await expect(
        manager.exportTrainingData({ minQuality: 8.0, outputDir: tempDir, runId: 'test' })
      ).rejects.toThrow('No eligible traces found');
    });
  });

  // ─── startFineTune ───

  describe('startFineTune', () => {
    it('returns a run with status "pending" immediately', async () => {
      await insertTestTraces(db, Array(60).fill({ quality_score: 9.0 }));

      const run = await manager.startFineTune();

      expect(run.status).toBe('pending');
      expect(run.id).toBeTruthy();
    });

    it('throws InsufficientTracesError when fewer than minTraces exist', async () => {
      await insertTestTraces(db, Array(30).fill({ quality_score: 9.0 }));

      await expect(manager.startFineTune()).rejects.toThrow('InsufficientTracesError');
    });

    it('throws when a run is already in progress', async () => {
      await insertTestTraces(db, Array(60).fill({ quality_score: 9.0 }));
      await manager.startFineTune();

      await expect(manager.startFineTune()).rejects.toThrow('already in progress');
    });

    it('persists run to database', async () => {
      await insertTestTraces(db, Array(60).fill({ quality_score: 9.0 }));
      const run = await manager.startFineTune();

      const stored = await manager.getRunStatus(run.id);
      expect(stored.id).toBe(run.id);
    });

    it('uses custom config when provided', async () => {
      await insertTestTraces(db, Array(60).fill({ quality_score: 9.0 }));
      const run = await manager.startFineTune({ loraRank: 32, epochs: 5 });

      expect(run.config.loraRank).toBe(32);
      expect(run.config.epochs).toBe(5);
    });
  });

  // ─── cancelFineTune ───

  describe('cancelFineTune', () => {
    it('marks run as cancelled', async () => {
      await insertTestTraces(db, Array(60).fill({ quality_score: 9.0 }));
      const run = await manager.startFineTune();

      await manager.cancelFineTune(run.id);

      const status = await manager.getRunStatus(run.id);
      expect(status.status).toBe('cancelled');
    });

    it('kills the subprocess', async () => {
      const killSpy = vi.spyOn(process, 'kill');
      await insertTestTraces(db, Array(60).fill({ quality_score: 9.0 }));
      const run = await manager.startFineTune();

      await manager.cancelFineTune(run.id);

      expect(killSpy).toHaveBeenCalled();
    });

    it('throws when run does not exist', async () => {
      await expect(manager.cancelFineTune('nonexistent')).rejects.toThrow('not found');
    });

    it('throws when run is already completed', async () => {
      const completedRun = await insertCompletedRun(db);
      await expect(manager.cancelFineTune(completedRun.id)).rejects.toThrow('already completed');
    });
  });

  // ─── parseProgressLine ───

  describe('parseProgressLine (private, tested via reflection)', () => {
    it('parses valid progress JSON', () => {
      const line = '{"type":"progress","step":10,"total_steps":270,"loss":1.42,"eval_loss":null,"learning_rate":0.0002,"eta_seconds":1620}';
      const event = (manager as any).parseProgressLine(line);

      expect(event).toMatchObject({
        type: 'progress',
        step: 10,
        totalSteps: 270,
        loss: 1.42
      });
    });

    it('returns null for non-JSON lines', () => {
      const event = (manager as any).parseProgressLine('Loading model...');
      expect(event).toBeNull();
    });

    it('returns null for JSON without type field', () => {
      const event = (manager as any).parseProgressLine('{"foo": "bar"}');
      expect(event).toBeNull();
    });

    it('parses completion event', () => {
      const line = '{"type":"complete","metrics":{"final_train_loss":0.42,"final_val_loss":0.51},"adapter_path":"./data/lora-adapters/run_abc"}';
      const event = (manager as any).parseProgressLine(line);

      expect(event.type).toBe('complete');
      expect(event.metrics.finalTrainLoss).toBe(0.42);
    });
  });

  // ─── listRuns ───

  describe('listRuns', () => {
    it('returns runs sorted by started_at descending', async () => {
      await insertFineTuneRuns(db, [
        { id: 'run-1', started_at: '2026-01-01' },
        { id: 'run-2', started_at: '2026-01-03' },
        { id: 'run-3', started_at: '2026-01-02' }
      ]);

      const runs = await manager.listRuns();

      expect(runs[0].id).toBe('run-2');
      expect(runs[1].id).toBe('run-3');
      expect(runs[2].id).toBe('run-1');
    });

    it('filters by status', async () => {
      await insertFineTuneRuns(db, [
        { id: 'run-1', status: 'completed' },
        { id: 'run-2', status: 'failed' },
        { id: 'run-3', status: 'completed' }
      ]);

      const runs = await manager.listRuns({ status: 'completed' });

      expect(runs).toHaveLength(2);
      expect(runs.every(r => r.status === 'completed')).toBe(true);
    });

    it('supports pagination', async () => {
      await insertFineTuneRuns(db, Array(25).fill({ status: 'completed' }));

      const page1 = await manager.listRuns({ limit: 10, offset: 0 });
      const page2 = await manager.listRuns({ limit: 10, offset: 10 });

      expect(page1).toHaveLength(10);
      expect(page2).toHaveLength(10);
      expect(page1[0].id).not.toBe(page2[0].id);
    });
  });
});
```

### 2.2 FineTuneScheduler Unit Tests

**File:** [`packages/core/src/finetune/scheduler.test.ts`](../packages/core/src/finetune/scheduler.test.ts)

```typescript
describe('FineTuneScheduler', () => {
  describe('checkConditions', () => {
    it('returns null when conditions are met', async () => {
      // 60 new traces since last run, min is 50
      const reason = await scheduler.checkConditions();
      expect(reason).toBeNull();
    });

    it('returns reason when not enough new traces', async () => {
      // Only 30 new traces since last run
      const reason = await scheduler.checkConditions();
      expect(reason).toContain('Insufficient new traces');
    });

    it('returns reason when a run is already in progress', async () => {
      vi.spyOn(manager, 'isRunning').mockReturnValue(true);
      const reason = await scheduler.checkConditions();
      expect(reason).toContain('already in progress');
    });

    it('returns reason when scheduler is disabled', async () => {
      scheduler.config.enabled = false;
      const reason = await scheduler.checkConditions();
      expect(reason).toContain('disabled');
    });
  });

  describe('start/stop', () => {
    it('starts the cron task', () => {
      scheduler.start();
      expect(scheduler.task).not.toBeNull();
    });

    it('stops the cron task', () => {
      scheduler.start();
      scheduler.stop();
      expect(scheduler.task).toBeNull();
    });

    it('does not start if already running', () => {
      scheduler.start();
      const task1 = scheduler.task;
      scheduler.start();
      expect(scheduler.task).toBe(task1);
    });
  });
});
```

### 2.3 ProgressTracker Unit Tests

**File:** [`packages/core/src/finetune/progress-tracker.test.ts`](../packages/core/src/finetune/progress-tracker.test.ts)

```typescript
describe('ProgressTracker', () => {
  describe('subscribe/emit', () => {
    it('delivers events to subscribers', () => {
      const mockReply = createMockReply();
      tracker.subscribe('run-1', mockReply);

      tracker.emit('run-1', {
        type: 'progress', runId: 'run-1', step: 10,
        totalSteps: 100, timestamp: new Date().toISOString()
      });

      expect(mockReply.raw.write).toHaveBeenCalledWith(
        expect.stringContaining('"step":10')
      );
    });

    it('does not deliver events to unsubscribed clients', () => {
      const mockReply = createMockReply();
      tracker.subscribe('run-1', mockReply);
      tracker.unsubscribe('run-1', mockReply);

      tracker.emit('run-1', { type: 'progress', runId: 'run-1', step: 10, totalSteps: 100, timestamp: '' });

      expect(mockReply.raw.write).not.toHaveBeenCalled();
    });

    it('delivers to multiple subscribers for the same run', () => {
      const reply1 = createMockReply();
      const reply2 = createMockReply();
      tracker.subscribe('run-1', reply1);
      tracker.subscribe('run-1', reply2);

      tracker.emit('run-1', { type: 'progress', runId: 'run-1', step: 1, totalSteps: 10, timestamp: '' });

      expect(reply1.raw.write).toHaveBeenCalled();
      expect(reply2.raw.write).toHaveBeenCalled();
    });

    it('stores latest progress for late subscribers', () => {
      tracker.emit('run-1', { type: 'progress', runId: 'run-1', step: 50, totalSteps: 100, timestamp: '' });

      const latest = tracker.getLatestProgress('run-1');
      expect(latest?.step).toBe(50);
    });
  });
});
```

### 2.4 Python: export_traces.py Tests

**File:** [`packages/finetune/tests/test_export_traces.py`](../packages/finetune/tests/test_export_traces.py)

```python
import pytest
import sqlite3
import json
import tempfile
from pathlib import Path
from src.export_traces import (
    get_eligible_traces,
    format_as_alpaca,
    deduplicate,
    split_train_val,
    write_jsonl
)

@pytest.fixture
def test_db(tmp_path):
    """Create a test SQLite database with sample traces."""
    db_path = tmp_path / "test.db"
    conn = sqlite3.connect(db_path)
    conn.execute("""
        CREATE TABLE traces (
            id TEXT PRIMARY KEY,
            query TEXT NOT NULL,
            final_answer TEXT NOT NULL,
            quality_score REAL,
            user_rating INTEGER,
            templates_used TEXT,
            proposer_model TEXT DEFAULT 'qwen3:32b',
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    """)
    conn.execute("""
        CREATE TABLE rounds (
            id TEXT PRIMARY KEY,
            trace_id TEXT NOT NULL,
            round_number INTEGER NOT NULL,
            proposer_response TEXT NOT NULL,
            skeptic_response TEXT NOT NULL
        )
    """)
    conn.commit()
    return db_path, conn

class TestGetEligibleTraces:
    def test_returns_traces_above_quality_threshold(self, test_db):
        db_path, conn = test_db
        conn.executemany(
            "INSERT INTO traces (id, query, final_answer, quality_score) VALUES (?, ?, ?, ?)",
            [
                ("t1", "Q1", "A1", 9.0),
                ("t2", "Q2", "A2", 8.0),
                ("t3", "Q3", "A3", 7.9),
                ("t4", "Q4", "A4", 5.0),
            ]
        )
        conn.commit()

        traces = list(get_eligible_traces(str(db_path), min_quality=8.0))
        assert len(traces) == 2
        assert all(t['quality_score'] >= 8.0 for t in traces)

    def test_includes_traces_with_high_user_rating(self, test_db):
        db_path, conn = test_db
        conn.execute(
            "INSERT INTO traces (id, query, final_answer, quality_score, user_rating) VALUES (?, ?, ?, ?, ?)",
            ("t1", "Q1", "A1", 6.0, 9)
        )
        conn.commit()

        traces = list(get_eligible_traces(str(db_path), min_quality=8.0))
        assert len(traces) == 1

    def test_returns_empty_when_no_eligible_traces(self, test_db):
        db_path, conn = test_db
        conn.execute(
            "INSERT INTO traces (id, query, final_answer, quality_score) VALUES (?, ?, ?, ?)",
            ("t1", "Q1", "A1", 5.0)
        )
        conn.commit()

        traces = list(get_eligible_traces(str(db_path), min_quality=8.0))
        assert len(traces) == 0

class TestFormatAsAlpaca:
    def test_formats_correctly(self):
        trace = {
            'query': 'Prove √2 is irrational',
            'final_answer': 'Assume √2 = p/q...',
            'templates_used': json.dumps(['proof-by-contradiction'])
        }
        example = format_as_alpaca(trace)

        assert example['instruction'] == 'Prove √2 is irrational'
        assert 'proof-by-contradiction' in example['input']
        assert example['output'] == 'Assume √2 = p/q...'

    def test_handles_null_templates(self):
        trace = {
            'query': 'Q', 'final_answer': 'A', 'templates_used': None
        }
        example = format_as_alpaca(trace)
        assert example['input'] == ''

    def test_handles_empty_templates_array(self):
        trace = {
            'query': 'Q', 'final_answer': 'A', 'templates_used': '[]'
        }
        example = format_as_alpaca(trace)
        assert example['input'] == ''

class TestDeduplicate:
    def test_removes_duplicate_queries(self):
        examples = [
            {'instruction': 'Same Q', 'input': '', 'output': 'A1', '_quality': 9.0},
            {'instruction': 'Same Q', 'input': '', 'output': 'A2', '_quality': 8.5},
            {'instruction': 'Different Q', 'input': '', 'output': 'A3', '_quality': 9.0},
        ]
        result = deduplicate(examples)
        assert len(result) == 2

    def test_keeps_highest_quality_duplicate(self):
        examples = [
            {'instruction': 'Q', 'input': '', 'output': 'A1', '_quality': 8.5},
            {'instruction': 'Q', 'input': '', 'output': 'A2', '_quality': 9.0},
        ]
        result = deduplicate(examples)
        assert result[0]['output'] == 'A2'

    def test_preserves_unique_examples(self):
        examples = [
            {'instruction': 'Q1', 'input': '', 'output': 'A1', '_quality': 9.0},
            {'instruction': 'Q2', 'input': '', 'output': 'A2', '_quality': 9.0},
        ]
        result = deduplicate(examples)
        assert len(result) == 2

class TestSplitTrainVal:
    def test_splits_90_10(self):
        examples = [{'instruction': f'Q{i}', 'input': '', 'output': f'A{i}'} for i in range(100)]
        train, val = split_train_val(examples, val_ratio=0.1, seed=42)
        assert len(train) == 90
        assert len(val) == 10

    def test_is_deterministic(self):
        examples = [{'instruction': f'Q{i}', 'input': '', 'output': f'A{i}'} for i in range(100)]
        train1, _ = split_train_val(examples, val_ratio=0.1, seed=42)
        train2, _ = split_train_val(examples, val_ratio=0.1, seed=42)
        assert [e['instruction'] for e in train1] == [e['instruction'] for e in train2]

    def test_different_seeds_produce_different_splits(self):
        examples = [{'instruction': f'Q{i}', 'input': '', 'output': f'A{i}'} for i in range(100)]
        train1, _ = split_train_val(examples, val_ratio=0.1, seed=42)
        train2, _ = split_train_val(examples, val_ratio=0.1, seed=123)
        assert [e['instruction'] for e in train1] != [e['instruction'] for e in train2]

    def test_no_overlap_between_train_and_val(self):
        examples = [{'instruction': f'Q{i}', 'input': '', 'output': f'A{i}'} for i in range(100)]
        train, val = split_train_val(examples, val_ratio=0.1, seed=42)
        train_instructions = {e['instruction'] for e in train}
        val_instructions = {e['instruction'] for e in val}
        assert len(train_instructions & val_instructions) == 0

class TestWriteJsonl:
    def test_writes_valid_jsonl(self, tmp_path):
        examples = [
            {'instruction': 'Q1', 'input': '', 'output': 'A1'},
            {'instruction': 'Q2', 'input': 'ctx', 'output': 'A2'},
        ]
        output_path = tmp_path / "output.jsonl"
        write_jsonl(examples, output_path)

        lines = output_path.read_text().strip().split('\n')
        assert len(lines) == 2
        assert json.loads(lines[0])['instruction'] == 'Q1'
        assert json.loads(lines[1])['instruction'] == 'Q2'

    def test_creates_parent_directories(self, tmp_path):
        output_path = tmp_path / "nested" / "dir" / "output.jsonl"
        write_jsonl([{'instruction': 'Q', 'input': '', 'output': 'A'}], output_path)
        assert output_path.exists()
```

### 2.5 API Route Tests: Fine-Tuning

**File:** [`packages/core/src/server/routes/finetune.routes.test.ts`](../packages/core/src/server/routes/finetune.routes.test.ts)

```typescript
describe('POST /api/finetune', () => {
  it('returns 202 with runId when conditions are met', async () => {
    mockTraceStore.getStats.mockResolvedValue({ fineTuneCandidates: 60 });
    mockManager.isRunning.mockReturnValue(false);
    mockManager.startFineTune.mockResolvedValue({ id: 'run-1', status: 'pending' });

    const response = await app.inject({
      method: 'POST',
      url: '/api/finetune'
    });

    expect(response.statusCode).toBe(202);
    expect(response.json()).toMatchObject({
      runId: 'run-1',
      status: 'pending',
      progressUrl: '/api/finetune/run-1/progress'
    });
  });

  it('returns 409 when a run is already in progress', async () => {
    mockManager.isRunning.mockReturnValue(true);

    const response = await app.inject({ method: 'POST', url: '/api/finetune' });

    expect(response.statusCode).toBe(409);
    expect(response.json().error).toContain('already in progress');
  });

  it('returns 422 when insufficient traces', async () => {
    mockTraceStore.getStats.mockResolvedValue({ fineTuneCandidates: 30 });
    mockManager.isRunning.mockReturnValue(false);

    const response = await app.inject({ method: 'POST', url: '/api/finetune' });

    expect(response.statusCode).toBe(422);
    expect(response.json()).toMatchObject({
      error: expect.stringContaining('Insufficient traces'),
      current: 30,
      required: 50
    });
  });

  it('accepts custom config in request body', async () => {
    mockTraceStore.getStats.mockResolvedValue({ fineTuneCandidates: 60 });
    mockManager.isRunning.mockReturnValue(false);
    mockManager.startFineTune.mockResolvedValue({ id: 'run-1', status: 'pending' });

    await app.inject({
      method: 'POST',
      url: '/api/finetune',
      payload: { config: { loraRank: 32, epochs: 5 } }
    });

    expect(mockManager.startFineTune).toHaveBeenCalledWith({ loraRank: 32, epochs: 5 });
  });
});

describe('DELETE /api/finetune/:runId', () => {
  it('cancels a running fine-tune', async () => {
    mockManager.getRunStatus.mockResolvedValue({ id: 'run-1', status: 'running' });

    const response = await app.inject({
      method: 'DELETE',
      url: '/api/finetune/run-1'
    });

    expect(response.statusCode).toBe(200);
    expect(mockManager.cancelFineTune).toHaveBeenCalledWith('run-1');
  });

  it('returns 404 for non-existent run', async () => {
    mockManager.getRunStatus.mockResolvedValue(null);

    const response = await app.inject({
      method: 'DELETE',
      url: '/api/finetune/nonexistent'
    });

    expect(response.statusCode).toBe(404);
  });
});
```

---

## 3. Feature 2: LoRA Version Management Tests

### 3.1 LoRAVersionManager Unit Tests

**File:** [`packages/core/src/lora/version-manager.test.ts`](../packages/core/src/lora/version-manager.test.ts)

```typescript
describe('LoRAVersionManager', () => {
  let manager: LoRAVersionManager;
  let adaptersDir: string;
  let db: Database;

  beforeEach(() => {
    adaptersDir = createTempDir();
    db = createTestDb();
    manager = new LoRAVersionManager(adaptersDir, mockOllamaClient, mockTraceStore);
  });

  // ─── listAdapters ───

  describe('listAdapters', () => {
    it('returns all adapters sorted by creation date descending', async () => {
      createMockAdapter(adaptersDir, { id: 'adapter-1', createdAt: '2026-01-01' });
      createMockAdapter(adaptersDir, { id: 'adapter-2', createdAt: '2026-01-03' });
      createMockAdapter(adaptersDir, { id: 'adapter-3', createdAt: '2026-01-02' });

      const adapters = await manager.listAdapters();

      expect(adapters[0].id).toBe('adapter-2');
      expect(adapters[1].id).toBe('adapter-3');
      expect(adapters[2].id).toBe('adapter-1');
    });

    it('marks corrupted adapters as invalid', async () => {
      createMockAdapter(adaptersDir, { id: 'good-adapter' });
      createMockAdapter(adaptersDir, { id: 'bad-adapter', corrupt: true });

      const adapters = await manager.listAdapters();
      const bad = adapters.find(a => a.id === 'bad-adapter');

      expect(bad?.isValid).toBe(false);
      expect(bad?.invalidReason).toBeTruthy();
    });

    it('returns empty array when no adapters exist', async () => {
      const adapters = await manager.listAdapters();
      expect(adapters).toHaveLength(0);
    });

    it('marks the active adapter correctly', async () => {
      createMockAdapter(adaptersDir, { id: 'adapter-1' });
      createMockAdapter(adaptersDir, { id: 'adapter-2' });
      await writeActivePointer(adaptersDir, 'adapter-1');

      const adapters = await manager.listAdapters();
      const active = adapters.find(a => a.id === 'adapter-1');
      const inactive = adapters.find(a => a.id === 'adapter-2');

      expect(active?.isActive).toBe(true);
      expect(inactive?.isActive).toBe(false);
    });
  });

  // ─── activateAdapter ───

  describe('activateAdapter', () => {
    it('updates active.json atomically', async () => {
      createMockAdapter(adaptersDir, { id: 'adapter-1' });

      await manager.activateAdapter('adapter-1', 'user');

      const pointer = JSON.parse(
        await fs.readFile(path.join(adaptersDir, 'active.json'), 'utf-8')
      );
      expect(pointer.adapterId).toBe('adapter-1');
    });

    it('updates manifest.json for old and new adapters', async () => {
      createMockAdapter(adaptersDir, { id: 'adapter-1' });
      createMockAdapter(adaptersDir, { id: 'adapter-2' });
      await writeActivePointer(adaptersDir, 'adapter-1');

      await manager.activateAdapter('adapter-2', 'user');

      const manifest1 = JSON.parse(
        await fs.readFile(path.join(adaptersDir, 'adapter-1', 'manifest.json'), 'utf-8')
      );
      const manifest2 = JSON.parse(
        await fs.readFile(path.join(adaptersDir, 'adapter-2', 'manifest.json'), 'utf-8')
      );

      expect(manifest1.isActive).toBe(false);
      expect(manifest2.isActive).toBe(true);
    });

    it('logs the activation event', async () => {
      createMockAdapter(adaptersDir, { id: 'adapter-1' });

      await manager.activateAdapter('adapter-1', 'user');

      const history = await manager.getActivationHistory();
      expect(history).toHaveLength(1);
      expect(history[0].adapterId).toBe('adapter-1');
      expect(history[0].triggeredBy).toBe('user');
    });

    it('throws AdapterNotFoundError for non-existent adapter', async () => {
      await expect(
        manager.activateAdapter('nonexistent', 'user')
      ).rejects.toThrow('AdapterNotFoundError');
    });

    it('throws InvalidAdapterError for corrupted adapter', async () => {
      createMockAdapter(adaptersDir, { id: 'bad-adapter', corrupt: true });

      await expect(
        manager.activateAdapter('bad-adapter', 'user')
      ).rejects.toThrow('InvalidAdapterError');
    });

    it('stores previous adapter ID in active.json', async () => {
      createMockAdapter(adaptersDir, { id: 'adapter-1' });
      createMockAdapter(adaptersDir, { id: 'adapter-2' });
      await writeActivePointer(adaptersDir, 'adapter-1');

      await manager.activateAdapter('adapter-2', 'user');

      const pointer = JSON.parse(
        await fs.readFile(path.join(adaptersDir, 'active.json'), 'utf-8')
      );
      expect(pointer.previousAdapterId).toBe('adapter-1');
    });
  });

  // ─── rollback ───

  describe('rollback', () => {
    it('activates the previous adapter', async () => {
      createMockAdapter(adaptersDir, { id: 'adapter-1' });
      createMockAdapter(adaptersDir, { id: 'adapter-2' });
      await writeActivePointer(adaptersDir, 'adapter-2', 'adapter-1');

      const result = await manager.rollback();

      expect(result?.id).toBe('adapter-1');
      const pointer = JSON.parse(
        await fs.readFile(path.join(adaptersDir, 'active.json'), 'utf-8')
      );
      expect(pointer.adapterId).toBe('adapter-1');
    });

    it('deactivates all adapters when no previous exists', async () => {
      createMockAdapter(adaptersDir, { id: 'adapter-1' });
      await writeActivePointer(adaptersDir, 'adapter-1', null);

      const result = await manager.rollback();

      expect(result).toBeNull();
      const pointer = JSON.parse(
        await fs.readFile(path.join(adaptersDir, 'active.json'), 'utf-8')
      );
      expect(pointer.adapterId).toBeNull();
    });

    it('logs rollback event with triggeredBy: rollback', async () => {
      createMockAdapter(adaptersDir, { id: 'adapter-1' });
      createMockAdapter(adaptersDir, { id: 'adapter-2' });
      await writeActivePointer(adaptersDir, 'adapter-2', 'adapter-1');

      await manager.rollback();

      const history = await manager.getActivationHistory();
      expect(history[0].triggeredBy).toBe('rollback');
    });
  });

  // ─── deleteAdapter ───

  describe('deleteAdapter', () => {
    it('removes adapter directory from filesystem', async () => {
      createMockAdapter(adaptersDir, { id: 'adapter-1' });

      await manager.deleteAdapter('adapter-1');

      const exists = await fs.access(path.join(adaptersDir, 'adapter-1'))
        .then(() => true).catch(() => false);
      expect(exists).toBe(false);
    });

    it('throws ActiveAdapterDeletionError for active adapter', async () => {
      createMockAdapter(adaptersDir, { id: 'adapter-1' });
      await writeActivePointer(adaptersDir, 'adapter-1');

      await expect(
        manager.deleteAdapter('adapter-1')
      ).rejects.toThrow('ActiveAdapterDeletionError');
    });

    it('throws AdapterNotFoundError for non-existent adapter', async () => {
      await expect(
        manager.deleteAdapter('nonexistent')
      ).rejects.toThrow('AdapterNotFoundError');
    });
  });

  // ─── verifyIntegrity ───

  describe('verifyIntegrity', () => {
    it('returns true for valid adapter', async () => {
      createMockAdapter(adaptersDir, { id: 'adapter-1' });

      const isValid = await manager.verifyIntegrity('adapter-1');

      expect(isValid).toBe(true);
    });

    it('returns false when adapter_model.bin is missing', async () => {
      createMockAdapter(adaptersDir, { id: 'adapter-1' });
      await fs.unlink(path.join(adaptersDir, 'adapter-1', 'adapter_model.bin'));

      const isValid = await manager.verifyIntegrity('adapter-1');

      expect(isValid).toBe(false);
    });

    it('returns false when SHA-256 hash does not match', async () => {
      createMockAdapter(adaptersDir, { id: 'adapter-1' });
      // Corrupt the file after creation
      await fs.writeFile(
        path.join(adaptersDir, 'adapter-1', 'adapter_model.bin'),
        'corrupted data'
      );

      const isValid = await manager.verifyIntegrity('adapter-1');

      expect(isValid).toBe(false);
    });
  });

  // ─── exportAdapter / importAdapter ───

  describe('exportAdapter', () => {
    it('creates a valid tar.gz archive', async () => {
      createMockAdapter(adaptersDir, { id: 'adapter-1' });
      const outputDir = createTempDir();

      const archivePath = await manager.exportAdapter('adapter-1', outputDir);

      expect(archivePath).toMatch(/\.tar\.gz$/);
      expect(await fs.access(archivePath).then(() => true).catch(() => false)).toBe(true);
    });

    it('throws for invalid adapter', async () => {
      createMockAdapter(adaptersDir, { id: 'bad-adapter', corrupt: true });

      await expect(
        manager.exportAdapter('bad-adapter', createTempDir())
      ).rejects.toThrow('InvalidAdapterError');
    });
  });

  describe('importAdapter', () => {
    it('imports a valid archive', async () => {
      createMockAdapter(adaptersDir, { id: 'adapter-1' });
      const outputDir = createTempDir();
      const archivePath = await manager.exportAdapter('adapter-1', outputDir);

      // Delete original and import
      await manager.deleteAdapter('adapter-1');
      const importedId = await manager.importAdapter(archivePath);

      expect(importedId).toBeTruthy();
      const adapter = await manager.getAdapter(importedId);
      expect(adapter).not.toBeNull();
    });

    it('throws ImportValidationError for invalid archive', async () => {
      const invalidArchive = path.join(createTempDir(), 'invalid.tar.gz');
      await fs.writeFile(invalidArchive, 'not a valid archive');

      await expect(
        manager.importAdapter(invalidArchive)
      ).rejects.toThrow('ImportValidationError');
    });

    it('generates new ID on collision', async () => {
      createMockAdapter(adaptersDir, { id: 'adapter-1' });
      const outputDir = createTempDir();
      const archivePath = await manager.exportAdapter('adapter-1', outputDir);

      // Import without deleting original
      const importedId = await manager.importAdapter(archivePath);

      expect(importedId).not.toBe('adapter-1');
    });
  });
});
```

---

## 4. Feature 3: Benchmarking Dashboard Tests

### 4.1 BenchmarkRunner Unit Tests

**File:** [`packages/core/src/benchmark/runner.test.ts`](../packages/core/src/benchmark/runner.test.ts)

```typescript
describe('BenchmarkRunner', () => {
  describe('startRun', () => {
    it('returns a run with status pending', async () => {
      const run = await runner.startRun({ type: 'quick', triggeredBy: 'manual' });

      expect(run.status).toBe('pending');
      expect(run.questionCount).toBe(10);
    });

    it('returns 409 when a benchmark is already running', async () => {
      await runner.startRun({ type: 'quick', triggeredBy: 'manual' });

      await expect(
        runner.startRun({ type: 'quick', triggeredBy: 'manual' })
      ).rejects.toThrow('already running');
    });

    it('uses 50 questions for full benchmark', async () => {
      const run = await runner.startRun({ type: 'full', triggeredBy: 'manual' });
      expect(run.questionCount).toBe(50);
    });

    it('uses custom question IDs for custom benchmark', async () => {
      const run = await runner.startRun({
        type: 'custom',
        customQuestionIds: ['bm-math-001', 'bm-code-001'],
        triggeredBy: 'manual'
      });
      expect(run.questionCount).toBe(2);
    });
  });

  describe('computeMetrics', () => {
    it('computes correct average quality score', () => {
      const results = [
        { qualityScore: 8.0, responseTimeMs: 30000, debateRounds: 3, earlyStopped: false },
        { qualityScore: 9.0, responseTimeMs: 25000, debateRounds: 2, earlyStopped: true },
        { qualityScore: 7.0, responseTimeMs: 45000, debateRounds: 4, earlyStopped: false },
      ];

      const metrics = (runner as any).computeMetrics(results, null, null);

      expect(metrics.avgQualityScore).toBeCloseTo(8.0);
    });

    it('computes per-domain scores', () => {
      const results = [
        { domain: 'mathematics', qualityScore: 9.0, responseTimeMs: 30000, debateRounds: 3, earlyStopped: false },
        { domain: 'mathematics', qualityScore: 7.0, responseTimeMs: 30000, debateRounds: 3, earlyStopped: false },
        { domain: 'coding', qualityScore: 8.0, responseTimeMs: 30000, debateRounds: 3, earlyStopped: false },
      ];

      const metrics = (runner as any).computeMetrics(results, null, null);

      expect(metrics.domainScores.mathematics).toBeCloseTo(8.0);
      expect(metrics.domainScores.coding).toBeCloseTo(8.0);
    });

    it('computes vsBaseline correctly', () => {
      const results = [{ qualityScore: 9.0, responseTimeMs: 30000, debateRounds: 3, earlyStopped: false }];
      const baselineMetrics = { tmdeScore: 60, avgQualityScore: 7.0 } as BenchmarkMetrics;

      const metrics = (runner as any).computeMetrics(results, baselineMetrics, null);

      expect(metrics.vsBaseline).toBeGreaterThan(0);
    });
  });

  describe('computeTMDEScore', () => {
    it('returns 100 for perfect quality and fast response', () => {
      const score = (runner as any).computeTMDEScore(10, 10000, 1);
      expect(score).toBe(100);
    });

    it('returns 0 for zero quality', () => {
      const score = (runner as any).computeTMDEScore(0, 300000, 5);
      expect(score).toBe(0);
    });

    it('penalizes slow responses', () => {
      const fastScore = (runner as any).computeTMDEScore(8, 30000, 3);
      const slowScore = (runner as any).computeTMDEScore(8, 300000, 3);
      expect(fastScore).toBeGreaterThan(slowScore);
    });

    it('penalizes more debate rounds', () => {
      const fewRounds = (runner as any).computeTMDEScore(8, 60000, 2);
      const manyRounds = (runner as any).computeTMDEScore(8, 60000, 5);
      expect(fewRounds).toBeGreaterThan(manyRounds);
    });
  });
});
```

### 4.2 AutoScorer Unit Tests

**File:** [`packages/core/src/benchmark/scorer.test.ts`](../packages/core/src/benchmark/scorer.test.ts)

```typescript
describe('AutoScorer', () => {
  describe('scoreAnswer', () => {
    it('returns a score between 0 and 10', async () => {
      mockOllamaClient.generate.mockResolvedValue(
        '{"overall_score": 8.5, "reasoning": "Good answer", "rubric_scores": {"correctness": 9, "clarity": 8}}'
      );

      const result = await scorer.scoreAnswer({
        question: 'Prove √2 is irrational',
        answer: 'Assume √2 = p/q...',
        referenceAnswer: 'Assume √2 = p/q...',
        rubric: 'Correctness (50%), Clarity (50%)'
      });

      expect(result.score).toBeGreaterThanOrEqual(0);
      expect(result.score).toBeLessThanOrEqual(10);
    });

    it('handles malformed judge response gracefully', async () => {
      mockOllamaClient.generate.mockResolvedValue('Not valid JSON');

      const result = await scorer.scoreAnswer({
        question: 'Q', answer: 'A', referenceAnswer: 'R', rubric: 'R'
      });

      // Should return a default score rather than throwing
      expect(result.score).toBeDefined();
    });

    it('includes reasoning in the result', async () => {
      mockOllamaClient.generate.mockResolvedValue(
        '{"overall_score": 7.0, "reasoning": "Missing key step", "rubric_scores": {}}'
      );

      const result = await scorer.scoreAnswer({
        question: 'Q', answer: 'A', referenceAnswer: 'R', rubric: 'R'
      });

      expect(result.reasoning).toBe('Missing key step');
    });
  });

  describe('buildJudgePrompt', () => {
    it('includes all required sections', () => {
      const prompt = (scorer as any).buildJudgePrompt({
        question: 'Test Q',
        answer: 'Test A',
        referenceAnswer: 'Ref A',
        rubric: 'Test rubric'
      });

      expect(prompt).toContain('Test Q');
      expect(prompt).toContain('Test A');
      expect(prompt).toContain('Ref A');
      expect(prompt).toContain('Test rubric');
    });
  });
});
```

---

## 5. Feature 4: Community Template Sharing Tests

### 5.1 RegistryClient Unit Tests

**File:** [`packages/core/src/community/registry-client.test.ts`](../packages/core/src/community/registry-client.test.ts)

```typescript
describe('RegistryClient', () => {
  describe('searchTemplates', () => {
    it('returns templates from registry', async () => {
      mockFetch.mockResolvedValue({
        ok: true,
        json: () => Promise.resolve({
          templates: [{ id: 'tmpl-1', name: 'Test Template' }],
          total: 1, page: 1, pageSize: 20
        })
      });

      const result = await client.searchTemplates({ query: 'test' });

      expect(result.templates).toHaveLength(1);
      expect(result.templates[0].name).toBe('Test Template');
    });

    it('falls back to cache when registry is unreachable', async () => {
      mockFetch.mockRejectedValue(new Error('Network error'));
      await writeCache(cacheDir, 'search:{}', {
        templates: [{ id: 'cached-1', name: 'Cached Template' }],
        total: 1, page: 1, pageSize: 20
      });

      const result = await client.searchTemplates({});

      expect(result.templates[0].id).toBe('cached-1');
      expect((result as any).fromCache).toBe(true);
    });

    it('throws when registry unreachable and no cache', async () => {
      mockFetch.mockRejectedValue(new Error('Network error'));

      await expect(client.searchTemplates({})).rejects.toThrow();
    });

    it('caches successful responses', async () => {
      mockFetch.mockResolvedValue({
        ok: true,
        json: () => Promise.resolve({ templates: [], total: 0, page: 1, pageSize: 20 })
      });

      await client.searchTemplates({});

      const cached = await readCache(cacheDir, 'search:{}');
      expect(cached).not.toBeNull();
    });
  });

  describe('publishTemplate', () => {
    it('requires API key', async () => {
      const clientWithoutKey = new RegistryClient({ registryUrl: 'http://test', cacheDir });

      await expect(
        clientWithoutKey.publishTemplate({ name: 'Test', description: '', domain: 'math', methodology: '', keywords: [], content: '---\nname: Test\n---\n## Steps\nStep 1' })
      ).rejects.toThrow('API key required');
    });

    it('sends template to registry', async () => {
      mockFetch.mockResolvedValue({
        ok: true,
        json: () => Promise.resolve({ id: 'tmpl-new', name: 'New Template' })
      });

      const result = await clientWithKey.publishTemplate({
        name: 'New Template',
        description: 'A test template',
        domain: 'mathematics',
        methodology: 'deductive',
        keywords: ['proof'],
        content: '---\nname: New Template\ndomain: mathematics\ndescription: A test\n---\n## Steps\nStep 1'
      });

      expect(result.id).toBe('tmpl-new');
    });
  });

  describe('isReachable', () => {
    it('returns true when registry responds', async () => {
      mockFetch.mockResolvedValue({ ok: true });
      expect(await client.isReachable()).toBe(true);
    });

    it('returns false when registry is unreachable', async () => {
      mockFetch.mockRejectedValue(new Error('Network error'));
      expect(await client.isReachable()).toBe(false);
    });
  });
});
```

### 5.2 Template Validation Tests

**File:** [`packages/core/src/community/template-validation.test.ts`](../packages/core/src/community/template-validation.test.ts)

```typescript
describe('validateTemplateStructure', () => {
  it('accepts valid template', () => {
    const content = `---
name: "Test Template"
domain: mathematics
description: "A test template"
---

## Steps

1. Step one
2. Step two
3. Step three

## Checklist
- [ ] Is the claim clear?
`;
    const errors = validateTemplateStructure(content);
    expect(errors).toHaveLength(0);
  });

  it('rejects template without frontmatter', () => {
    const content = '## Steps\n\nStep 1';
    const errors = validateTemplateStructure(content);
    expect(errors).toContain(expect.stringContaining('frontmatter'));
  });

  it('rejects template without name in frontmatter', () => {
    const content = '---\ndomain: math\n---\n## Steps\nStep 1';
    const errors = validateTemplateStructure(content);
    expect(errors).toContain(expect.stringContaining('"name"'));
  });

  it('rejects template with body shorter than 100 characters', () => {
    const content = '---\nname: T\ndomain: math\ndescription: D\n---\nShort';
    const errors = validateTemplateStructure(content);
    expect(errors).toContain(expect.stringContaining('100 characters'));
  });

  it('rejects template without section headers', () => {
    const content = `---
name: "Test"
domain: math
description: "Test"
---

This is a template without any section headers. It has enough content to pass the length check but no markdown headers.
`;
    const errors = validateTemplateStructure(content);
    expect(errors).toContain(expect.stringContaining('section'));
  });
});

describe('detectPII', () => {
  it('detects email addresses', () => {
    const warnings = detectPII('Contact john.doe@example.com for help');
    expect(warnings.length).toBeGreaterThan(0);
  });

  it('detects phone numbers', () => {
    const warnings = detectPII('Call 555-123-4567 for support');
    expect(warnings.length).toBeGreaterThan(0);
  });

  it('detects IP addresses', () => {
    const warnings = detectPII('Server at 192.168.1.100');
    expect(warnings.length).toBeGreaterThan(0);
  });

  it('returns empty for clean content', () => {
    const warnings = detectPII('This is a clean template with no PII');
    expect(warnings).toHaveLength(0);
  });
});

describe('renderTemplateSafely', () => {
  it('renders markdown to HTML', () => {
    const html = renderTemplateSafely('## Title\n\nParagraph');
    expect(html).toContain('<h2>');
    expect(html).toContain('<p>');
  });

  it('strips script tags', () => {
    const html = renderTemplateSafely('<script>alert("xss")</script>');
    expect(html).not.toContain('<script>');
    expect(html).not.toContain('alert');
  });

  it('strips onclick attributes', () => {
    const html = renderTemplateSafely('<p onclick="evil()">Text</p>');
    expect(html).not.toContain('onclick');
  });

  it('allows safe markdown elements', () => {
    const html = renderTemplateSafely('**bold** _italic_ `code`');
    expect(html).toContain('<strong>');
    expect(html).toContain('<em>');
    expect(html).toContain('<code>');
  });
});
```

---

## 6. Integration Tests

### 6.1 Fine-Tuning Pipeline E2E

**File:** [`packages/core/src/finetune/pipeline.integration.test.ts`](../packages/core/src/finetune/pipeline.integration.test.ts)

```typescript
describe('Fine-Tuning Pipeline Integration', () => {
  it('completes full pipeline: export → train → save adapter', async () => {
    // This test uses a mock Python subprocess that simulates training
    const mockPythonScript = createMockTrainingScript({
      steps: 10,
      finalLoss: 0.42,
      adapterPath: path.join(tempDir, 'mock-adapter')
    });

    await insertTestTraces(db, Array(60).fill({ quality_score: 9.0 }));

    const run = await manager.startFineTune();

    // Wait for completion (with timeout)
    await waitForRunCompletion(manager, run.id, { timeoutMs: 30000 });

    const completedRun = await manager.getRunStatus(run.id);
    expect(completedRun.status).toBe('completed');
    expect(completedRun.loraPath).toBeTruthy();
    expect(completedRun.metrics?.finalTrainLoss).toBe(0.42);
  }, 35000);

  it('handles training process crash gracefully', async () => {
    const crashingScript = createCrashingTrainingScript();
    await insertTestTraces(db, Array(60).fill({ quality_score: 9.0 }));

    const run = await manager.startFineTune();
    await waitForRunCompletion(manager, run.id, { timeoutMs: 10000 });

    const failedRun = await manager.getRunStatus(run.id);
    expect(failedRun.status).toBe('failed');
    expect(failedRun.errorMessage).toBeTruthy();
  }, 15000);

  it('streams progress events via SSE', async () => {
    const progressEvents: ProgressEvent[] = [];
    const mockScript = createMockTrainingScript({ steps: 5 });
    await insertTestTraces(db, Array(60).fill({ quality_score: 9.0 }));

    const run = await manager.startFineTune();
    manager.on('progress', (event: ProgressEvent) => {
      if (event.runId === run.id) progressEvents.push(event);
    });

    await waitForRunCompletion(manager, run.id, { timeoutMs: 10000 });

    expect(progressEvents.length).toBeGreaterThan(0);
    expect(progressEvents.some(e => e.type === 'complete')).toBe(true);
  }, 15000);
});
```

### 6.2 LoRA Version Management Integration

**File:** [`packages/core/src/lora/version-management.integration.test.ts`](../packages/core/src/lora/version-management.integration.test.ts)

```typescript
describe('LoRA Version Management Integration', () => {
  it('rollback chain: A → B → C, rollback twice returns A', async () => {
    createMockAdapter(adaptersDir, { id: 'adapter-a' });
    createMockAdapter(adaptersDir, { id: 'adapter-b' });
    createMockAdapter(adaptersDir, { id: 'adapter-c' });

    await manager.activateAdapter('adapter-a', 'user');
    await manager.activateAdapter('adapter-b', 'user');
    await manager.activateAdapter('adapter-c', 'user');

    await manager.rollback(); // Should go to B
    const afterFirstRollback = await manager.getActiveAdapter();
    expect(afterFirstRollback?.id).toBe('adapter-b');

    await manager.rollback(); // Should go to A
    const afterSecondRollback = await manager.getActiveAdapter();
    expect(afterSecondRollback?.id).toBe('adapter-a');
  });

  it('export/import round-trip preserves all metadata', async () => {
    createMockAdapter(adaptersDir, {
      id: 'adapter-1',
      baseModel: 'qwen3:32b',
      metrics: { finalTrainLoss: 0.42, finalValLoss: 0.51 }
    });

    const exportDir = createTempDir();
    const archivePath = await manager.exportAdapter('adapter-1', exportDir);

    await manager.deleteAdapter('adapter-1');
    const importedId = await manager.importAdapter(archivePath);

    const imported = await manager.getAdapter(importedId);
    expect(imported?.baseModel).toBe('qwen3:32b');
    expect(imported?.metrics.finalTrainLoss).toBe(0.42);
  });
});
```

---

## 7. E2E Tests (Playwright)

**File:** [`tests/e2e/v0.3-self-improvement.spec.ts`](../tests/e2e/v0.3-self-improvement.spec.ts)

```typescript
import { test, expect } from '@playwright/test';

test.describe('Fine-Tuning Dashboard', () => {
  test('shows training data statistics', async ({ page }) => {
    await page.goto('/finetune');
    await expect(page.getByText('Training Data')).toBeVisible();
    await expect(page.getByTestId('total-traces')).toBeVisible();
    await expect(page.getByTestId('eligible-traces')).toBeVisible();
  });

  test('Run Now button triggers fine-tune', async ({ page }) => {
    await page.goto('/finetune');
    await page.getByRole('button', { name: 'Run Now' }).click();

    // Should show confirmation dialog
    await expect(page.getByRole('dialog')).toBeVisible();
    await page.getByRole('button', { name: 'Confirm' }).click();

    // Should show progress indicator
    await expect(page.getByTestId('training-progress')).toBeVisible();
  });
});

test.describe('LoRA Version Manager', () => {
  test('lists all adapter versions', async ({ page }) => {
    await page.goto('/finetune/versions');
    await expect(page.getByTestId('adapter-list')).toBeVisible();
  });

  test('activate button changes active adapter', async ({ page }) => {
    await page.goto('/finetune/versions');
    const firstAdapter = page.getByTestId('adapter-item').first();
    await firstAdapter.getByRole('button', { name: 'Activate' }).click();
    await page.getByRole('button', { name: 'Confirm' }).click();
    await expect(firstAdapter.getByTestId('active-badge')).toBeVisible();
  });
});

test.describe('Benchmarking Dashboard', () => {
  test('displays TMDE Score', async ({ page }) => {
    await page.goto('/benchmark');
    await expect(page.getByTestId('tmde-score')).toBeVisible();
  });

  test('Run Benchmark button starts a benchmark', async ({ page }) => {
    await page.goto('/benchmark');
    await page.getByRole('button', { name: 'Run Benchmark' }).click();
    await expect(page.getByTestId('benchmark-progress')).toBeVisible();
  });
});

test.describe('Community Hub', () => {
  test('displays community templates', async ({ page }) => {
    await page.goto('/community');
    await expect(page.getByTestId('template-list')).toBeVisible();
  });

  test('search filters templates', async ({ page }) => {
    await page.goto('/community');
    await page.getByPlaceholder('Search templates...').fill('mathematics');
    await expect(page.getByTestId('template-item')).toHaveCount(
      await page.getByTestId('template-item').count()
    );
  });

  test('import button imports a template', async ({ page }) => {
    await page.goto('/community');
    const firstTemplate = page.getByTestId('template-item').first();
    await firstTemplate.getByRole('button', { name: 'Import' }).click();
    await expect(page.getByText('Template imported successfully')).toBeVisible();
  });

  test('shows offline banner when registry unreachable', async ({ page }) => {
    // Mock network failure
    await page.route('**/api/community/templates', route => route.abort());
    await page.goto('/community');
    await expect(page.getByTestId('offline-banner')).toBeVisible();
  });
});
```

---

## 8. Test Coverage Requirements

| Module | Minimum Coverage |
|--------|-----------------|
| `packages/core/src/finetune/manager.ts` | 90% |
| `packages/core/src/finetune/scheduler.ts` | 85% |
| `packages/core/src/finetune/progress-tracker.ts` | 85% |
| `packages/core/src/lora/version-manager.ts` | 90% |
| `packages/core/src/benchmark/runner.ts` | 85% |
| `packages/core/src/benchmark/scorer.ts` | 85% |
| `packages/core/src/benchmark/questions.ts` | 80% |
| `packages/core/src/community/registry-client.ts` | 85% |
| `packages/finetune/src/export_traces.py` | 90% |
| `packages/finetune/src/train_unsloth.py` | 75% |
| API routes (all new endpoints) | 90% |

---

## 9. Test Data Fixtures

### 9.1 Sample Traces for Testing

```typescript
// packages/core/src/test-utils/fixtures.ts

export const SAMPLE_TRACES = [
  {
    id: 'trace-001',
    query: 'Prove that √2 is irrational',
    final_answer: 'Assume √2 = p/q in lowest terms...',
    quality_score: 9.2,
    user_rating: null,
    templates_used: JSON.stringify(['proof-by-contradiction']),
    proposer_model: 'qwen3:32b',
    skeptic_model: 'llama3.3:70b',
    total_rounds: 3,
    created_at: '2026-01-15T10:00:00Z'
  },
  {
    id: 'trace-002',
    query: 'Design a distributed rate limiter',
    final_answer: 'Use a token bucket algorithm with Redis...',
    quality_score: 8.7,
    user_rating: 9,
    templates_used: JSON.stringify(['system-design-decomposition']),
    proposer_model: 'qwen3:32b',
    skeptic_model: 'llama3.3:70b',
    total_rounds: 4,
    created_at: '2026-01-16T14:00:00Z'
  }
];

export const SAMPLE_BENCHMARK_QUESTIONS = [
  {
    id: 'bm-test-001',
    domain: 'mathematics',
    difficulty: 'medium',
    question: 'What is the sum of angles in a triangle?',
    reference_answer: '180 degrees. This can be proven by drawing a line parallel to one side through the opposite vertex.',
    scoring_rubric: 'Correctness (60%), Explanation (40%)',
    is_built_in: false
  }
];
```

---

## 10. CI/CD Integration

### 10.1 GitHub Actions Workflow Addition

```yaml
# .github/workflows/test-v0.3.yml
name: v0.3 Tests

on:
  push:
    paths:
      - 'packages/core/src/finetune/**'
      - 'packages/core/src/lora/**'
      - 'packages/core/src/benchmark/**'
      - 'packages/core/src/community/**'
      - 'packages/finetune/**'
      - 'apps/web/app/finetune/**'
      - 'apps/web/app/benchmark/**'
      - 'apps/web/app/community/**'

jobs:
  typescript-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with: { node-version: '20' }
      - run: npm install
      - run: npm run test:coverage --workspace=@tmde/core
      - name: Check coverage thresholds
        run: npm run test:coverage:check --workspace=@tmde/core

  python-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: '3.11' }
      - run: pip install -r packages/finetune/requirements.txt
      - run: pip install pytest pytest-cov
      - run: pytest packages/finetune/tests/ --cov=packages/finetune/src --cov-report=xml
      - name: Check Python coverage
        run: coverage report --fail-under=85
```
